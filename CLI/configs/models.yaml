router:
  name: codegemma-2b
  base_url: 'http://127.0.0.1:8081/v1'
  server_path: 'C:\llama.cpp\server.exe'
  model_path: 'C:\Users\usuario\Business\CLI\models\google\codegemma-2b\codegemma-2b_Q8_0.gguf'
  n_ctx: 8192          # 8K OK en CPU
  ngl: 0               # CPU para dejar la GPU libre
  threads: 14          # tienes 14 lógicos
  model_id: 'codegemma-2b_Q8_0'
  params: { temperature: 0.2, top_p: 0.9, max_tokens: 1024 }

generator_primary:
  name: qwen2.5-coder-7b
  base_url: 'http://127.0.0.1:8082/v1'
  server_path: 'C:\llama.cpp\server.exe'
  model_path: 'C:\Users\usuario\Business\CLI\models\Qwen\Qwen2.5-Coder-7B-Instruct\Qwen2.5-Coder-7B-Instruct_Q5_K_M.gguf'
  n_ctx: 4096          # 4K seguro en 12GB; sube a 8192 si lo necesitas
  ngl: 999             # offload total si tu server.exe tiene backend GPU (CUDA/Vulkan/SYCL)
  threads: 14
  n_batch: 384         # tamaño de lote para no OOM en 12GB
  n_ubatch: 256
  model_id: 'Qwen2.5-Coder-7B-Instruct_Q5_K_M'
  params: { temperature: 0.2, top_p: 0.8, max_tokens: 2048 }

assistant_qc:
  name: deepseek-coder-7b
  base_url: 'http://127.0.0.1:8083/v1'
  server_path: 'C:\llama.cpp\server.exe'
  model_path: 'C:\Users\usuario\Business\CLI\models\deepseek-ai\deepseek-coder-7b-instruct-v1.5\DeepSeek-Coder-7B-Instruct-v1.5_Q8_0.gguf'
  n_ctx: 4096
  ngl: 999
  threads: 14
  n_batch: 320         # DeepSeek Q8_0 es un poco más pesado
  n_ubatch: 192
  model_id: 'DeepSeek-Coder-7B-Instruct-v1.5_Q8_0'
  params: { temperature: 0.4, top_p: 0.9, max_tokens: 1024 }
